{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinija2006-nimmala/IIITH-IHUB-AI-ML-Course/blob/main/Copy_of_STP_Module_3_Lab_2_Implementing_KNN_from_scratch_and_visualize_Algorithm_performance_by_Om_Kathalkar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lNmnA0_AhlE"
      },
      "source": [
        "# **Student Training Program on AIML**\n",
        "### MODULE 3: CLASSIFICATION-1\n",
        "### LAB-2 : Implementing KNN from scratch and visualize Algorithm performance\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7v6N5-LBHWS"
      },
      "source": [
        "# **Section 1: Implementing KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIKaq_foFSXD"
      },
      "source": [
        "In the last lab we had started discussing about KNN or K Nearest Neighbour method for clasification. We used the pre-built scikit-learn library for KNN. Now let's see how to implement this algorithm from scratch  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "A7877e44n6Kd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nHzdQE_8oPM1"
      },
      "outputs": [],
      "source": [
        "def predict(X_train, y_train, X_test, k):\n",
        "    distances = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        # compute and store L2 distance\n",
        "        distances.append([np.sqrt(np.sum(np.square(X_test - X_train[i, :]))), i])\n",
        "\n",
        "    distances = sorted(distances)\n",
        "\n",
        "    for i in range(k):\n",
        "        index = distances[i][1]\n",
        "        targets.append(y_train[index])\n",
        "\n",
        "    # return most common target\n",
        "    return Counter(targets).most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9_kulEM0oWJ-"
      },
      "outputs": [],
      "source": [
        "def k_nearest_neighbor(X_train, y_train, X_test, k):\n",
        "\n",
        "    assert k <= len(X_train), \"[!] K cannot be larger than number of samples.\"\n",
        "\n",
        "    # loop over all observations\n",
        "    predictions = []\n",
        "    for i in range(len(X_test)):\n",
        "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
        "\n",
        "    return np.asarray(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_saK1gP2zd1"
      },
      "source": [
        "**Note** : If k = 1 then the algorithm will simply return the label of the nearest neighbour. When we give k > 1 the most common label out of the given labels in the k neighbours will be selected.The code for 1 NN is given as follows and does not have to be so complicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "wa4lAr9p3Tqk"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97MTzRbMKHfl"
      },
      "source": [
        "## Section 1.1 - Iris Dataset\n",
        "Let's try it out on Iris Dataset present in the scikit learn library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VJ7YJCT1KN29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ce0c4318-0803-4d4f-a3d3-8ce8cc5c4f10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target  \n",
              "0       0  \n",
              "1       0  \n",
              "2       0  \n",
              "3       0  \n",
              "4       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d81acb70-9469-4bfa-88a7-4611b3b0550e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d81acb70-9469-4bfa-88a7-4611b3b0550e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d81acb70-9469-4bfa-88a7-4611b3b0550e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d81acb70-9469-4bfa-88a7-4611b3b0550e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c57d1147-bbc9-41b3-be06-981f726c20ad\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c57d1147-bbc9-41b3-be06-981f726c20ad')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c57d1147-bbc9-41b3-be06-981f726c20ad button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"sepal length (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal width (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.435866284936698,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal length (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7652982332594667,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal width (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7622376689603465,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "r8SHprUbKtuI"
      },
      "outputs": [],
      "source": [
        "X = np.array(df[['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']])\n",
        "y = np.array(df['target'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "SUYb80o6LHUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c5f8c1-2c4f-4f08-87a6-a723bb9c1fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of our classifier is 94.0 %\n"
          ]
        }
      ],
      "source": [
        "# Making our predictions\n",
        "predictions = k_nearest_neighbor(X_train, y_train, X_test, 7)\n",
        "\n",
        "# evaluating accuracy\n",
        "accuracy = Accuracy(y_test, predictions)\n",
        "print(\"The accuracy of our classifier is {} %\".format(100*accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwQ3qP-UXY5x"
      },
      "source": [
        "Voila! You have implemented your own version of the K-Nearest Neighbours algorithm, which works very well on the Iris Dataset. Congratulations!  \n",
        "\n",
        "Now try out the sklearn implementation and compare your results.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "c0jJj5_7dg-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0cf878-c723-42d4-df9e-0c8c53f16787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of Sklearn classifier is 94.0 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "sklearn_knn = KNeighborsClassifier(n_neighbors=7)\n",
        "sklearn_knn.fit(X_train,y_train)\n",
        "sklearn_predictions = sklearn_knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, sklearn_predictions)\n",
        "print(\"The accuracy of Sklearn classifier is {} %\".format(100*accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yi2tX6h0_Ka"
      },
      "source": [
        "##  Section 1.2: Weighted NN\n",
        "For KNN, If k is too small, the algorithm would be more sensitive to outliers. If k is too large, then the neighborhood may include too many points from other classes. We also take the label with max votes in the neighbourhood. Another choice can be that instead of giving equal weights to each of the neighbours we can give weighted points in the neighbourhood. So we weigh points by the inverse of their distance. Therefore, closer points will be given a higher priority as compared to the far off points.\n",
        "\n",
        "An easy way to implement this is by specifying the 'weights' parameter as distance when defining the sklearn KNN function. For more information go through this [site](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "2hXZP4wXqtmK"
      },
      "outputs": [],
      "source": [
        "## TASK\n",
        "## Modify the KNN function you wrote to return all the K-nearest neighbours along with their distances,\n",
        "## instead of just the output that was most common. You don't need to find out accuracy, just modify the function\n",
        "## and return the k-nearest neighbours and distances."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modified KNN Function\n",
        "This function calculates the Euclidean distance between the test point and all training points.\n",
        "Instead of predicting the most common class, it returns the k-nearest neighbours along with:\n",
        "their distance from the test point,\n",
        "their index in the training set,\n",
        "and their corresponding label.\n"
      ],
      "metadata": {
        "id": "-rjtGivg4Eui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def knn_k_neighbours(X_train, y_train, x_test, k):\n",
        "    \"\"\"\n",
        "    Return the k-nearest neighbours and their distances.\n",
        "\n",
        "    Parameters:\n",
        "        X_train : np.ndarray, shape (n_samples, n_features)\n",
        "        y_train : np.ndarray or list, shape (n_samples,)\n",
        "        x_test  : np.ndarray, shape (n_features,)\n",
        "        k       : int, number of neighbours\n",
        "\n",
        "    Returns:\n",
        "        neighbours : list of dicts\n",
        "            Each dict has:\n",
        "                'index'    : index in X_train\n",
        "                'distance' : distance to x_test\n",
        "                'label'    : corresponding y_train value\n",
        "    \"\"\"\n",
        "    distances = []\n",
        "\n",
        "    # Compute distance from x_test to every training point\n",
        "    for i in range(len(X_train)):\n",
        "        d = np.linalg.norm(X_train[i] - x_test)  # Euclidean distance\n",
        "        distances.append((i, d, y_train[i]))\n",
        "\n",
        "    # Sort by distance (smallest first)\n",
        "    distances.sort(key=lambda t: t[1])\n",
        "\n",
        "    # Take first k as nearest neighbours\n",
        "    k_neighbours = distances[:k]\n",
        "\n",
        "    # Optional: make it a bit more readable\n",
        "    neighbours = [\n",
        "        {\n",
        "            \"index\": idx,\n",
        "            \"distance\": dist,\n",
        "            \"label\": lbl\n",
        "        }\n",
        "        for (idx, dist, lbl) in k_neighbours\n",
        "    ]\n",
        "\n",
        "    return neighbours\n",
        "X_train = np.array([[1, 2],\n",
        "                    [2, 3],\n",
        "                    [3, 4],\n",
        "                    [8, 9]])\n",
        "\n",
        "y_train = np.array([0, 0, 1, 1])\n",
        "\n",
        "x_test = np.array([2, 2])\n",
        "k = 3\n",
        "\n",
        "neighbors = knn_k_neighbours(X_train, y_train, x_test, k)\n",
        "print(neighbors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aD6T9s_3hhL",
        "outputId": "0e126bed-aa32-454c-ca2f-89dd7d2780b1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'index': 0, 'distance': np.float64(1.0), 'label': np.int64(0)}, {'index': 1, 'distance': np.float64(1.0), 'label': np.int64(0)}, {'index': 2, 'distance': np.float64(2.23606797749979), 'label': np.int64(1)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxcBnLl8GHWz"
      },
      "source": [
        "# **Section 2: Visualizing Data**  \n",
        "\n",
        "We will look into something called **Voronoi** diagrams.  \n",
        "\n",
        "**Note**: Ideally, we should perform data visualization to see what the data looks like before we apply any Machine Learning algorithm.  Only for the purpose of this lab session, we're explaining it after you've applied KNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aDsDfmXG79k"
      },
      "source": [
        "### Voronoi Diagrams  \n",
        "\n",
        "In simple terms, Voronoi diagrams help you to visualize the dataset by partioning the plane into regions that are close to a given set of points. These regions are also called Voronoi cells.  \n",
        "\n",
        "Note that the cells/regions depend on the Distance metric being used. One way of interpreting this is by understanding that the distance metric decides the degree to which a 'point' or 'seed' in the Voronoi diagram has influence.  For each seed there is a corresponding region, called a Voronoi cell, consisting of all points of the plane closer to that seed than to any other.\n",
        "\n",
        "This [link](https://en.wikipedia.org/wiki/Voronoi_diagram#Illustration) provides a wonderful illustration of Voronoi plots for 20 points in two cases: (1) Using Euclidean distance, and (2) Using Manhattan distance.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdcdjthOKwra"
      },
      "source": [
        "Since our Iris data is 4-dimensional (as it has 4 attributes), we need to convert into a form that can be represented in 2-D.   \n",
        "\n",
        "While there are methods to visualize data higher than 2-dimensions, that is beyond scope for now.  \n",
        "\n",
        "For simplicity, we just take the first two columns of the iris dataset attributes and observe the Voronoi diagram generated for that.  \n",
        "Alternatively, one can also perform PCA (Principal Component Analysis), to reduce the 4D data to just two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ZNOPcS3f7sZg"
      },
      "outputs": [],
      "source": [
        "#@title Plotting Voronoi regions\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "\n",
        "def voronoi_finite_polygons_2d(vor, radius=None):\n",
        "    \"\"\"\n",
        "    Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
        "    regions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vor : Voronoi\n",
        "        Input diagram\n",
        "    radius : float, optional\n",
        "        Distance to 'points at infinity'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    regions : list of tuples\n",
        "        Indices of vertices in each revised Voronoi regions.\n",
        "    vertices : list of tuples\n",
        "        Coordinates for revised Voronoi vertices. Same as coordinates\n",
        "        of input vertices, with 'points at infinity' appended to the\n",
        "        end.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if vor.points.shape[1] != 2:\n",
        "        raise ValueError(\"Requires 2D input\")\n",
        "\n",
        "    new_regions = []\n",
        "    new_vertices = vor.vertices.tolist()\n",
        "\n",
        "    center = vor.points.mean(axis=0)\n",
        "    if radius is None:\n",
        "        radius = vor.points.ptp().max()\n",
        "\n",
        "    # Construct a map containing all ridges for a given point\n",
        "    all_ridges = {}\n",
        "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
        "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
        "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
        "\n",
        "    # Reconstruct infinite regions\n",
        "    for p1, region in enumerate(vor.point_region):\n",
        "        vertices = vor.regions[region]\n",
        "\n",
        "        if all(v >= 0 for v in vertices):\n",
        "            # finite region\n",
        "            new_regions.append(vertices)\n",
        "            continue\n",
        "\n",
        "        # reconstruct a non-finite region\n",
        "        ridges = all_ridges[p1]\n",
        "        new_region = [v for v in vertices if v >= 0]\n",
        "\n",
        "        for p2, v1, v2 in ridges:\n",
        "            if v2 < 0:\n",
        "                v1, v2 = v2, v1\n",
        "            if v1 >= 0:\n",
        "                # finite ridge: already in the region\n",
        "                continue\n",
        "\n",
        "            # Compute the missing endpoint of an infinite ridge\n",
        "\n",
        "            t = vor.points[p2] - vor.points[p1] # tangent\n",
        "            t /= np.linalg.norm(t)\n",
        "            n = np.array([-t[1], t[0]])  # normal\n",
        "\n",
        "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
        "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
        "            far_point = vor.vertices[v2] + direction * radius\n",
        "\n",
        "            new_region.append(len(new_vertices))\n",
        "            new_vertices.append(far_point.tolist())\n",
        "\n",
        "        # sort region counterclockwise\n",
        "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
        "        c = vs.mean(axis=0)\n",
        "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
        "        new_region = np.array(new_region)[np.argsort(angles)]\n",
        "\n",
        "        # finish\n",
        "        new_regions.append(new_region.tolist())\n",
        "\n",
        "    return new_regions, np.asarray(new_vertices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "9NIVhxz8KvG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "aac2e324-6a4e-4350-a400-b40f28c2be49"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "`ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2825929352.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoronoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mregions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoronoi_finite_polygons_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3671354564.py\u001b[0m in \u001b[0;36mvoronoi_finite_polygons_2d\u001b[0;34m(vor, radius)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mradius\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Construct a map containing all ridges for a given point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `ptp` was removed from the ndarray class in NumPy 2.0. Use np.ptp(arr, ...) instead."
          ]
        }
      ],
      "source": [
        "points = []\n",
        "xpts = np.array(df['sepal length (cm)'])\n",
        "ypts = np.array(df['sepal width (cm)'])\n",
        "for i in range(len(xpts)):\n",
        "  points.append([xpts[i],ypts[i]])\n",
        "\n",
        "points = np.array(points)\n",
        "vor = Voronoi(points)\n",
        "\n",
        "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
        "\n",
        "for region in regions:\n",
        "    polygon = vertices[region]\n",
        "    plt.fill(*zip(*polygon), alpha=0.4)\n",
        "\n",
        "plt.plot(points[:,0], points[:,1], 'ko')\n",
        "plt.xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)\n",
        "plt.ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def voronoi_finite_polygons_2d(vor, radius=None):\n",
        "    \"\"\"\n",
        "    Reconstruct infinite Voronoi regions in a 2D diagram to finite regions.\n",
        "    Modified to work with NumPy 2.0 (uses np.ptp instead of ndarray.ptp()).\n",
        "    \"\"\"\n",
        "    if vor.points.shape[1] != 2:\n",
        "        raise ValueError(\"Requires 2D input\")\n",
        "\n",
        "    new_regions = []\n",
        "    new_vertices = vor.vertices.tolist()\n",
        "    center = vor.points.mean(axis=0)\n",
        "\n",
        "    # FIX: Use np.ptp instead of vor.points.ptp()\n",
        "    if radius is None:\n",
        "        radius = np.ptp(vor.points, axis=0).max()\n",
        "\n",
        "    all_ridges = {}\n",
        "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
        "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
        "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
        "\n",
        "    for p1, region in enumerate(vor.point_region):\n",
        "        vertices = vor.regions[region]\n",
        "\n",
        "        if all(v >= 0 for v in vertices):\n",
        "            new_regions.append(vertices)\n",
        "            continue\n",
        "\n",
        "        ridges = all_ridges[p1]\n",
        "        new_region = []\n",
        "\n",
        "        for p2, v1, v2 in ridges:\n",
        "            v1, v2 = int(v1), int(v2)\n",
        "\n",
        "            if v1 >= 0:\n",
        "                new_region.append(v1)\n",
        "            if v2 >= 0:\n",
        "                new_region.append(v2)\n",
        "\n",
        "            if v1 < 0 or v2 < 0:\n",
        "                t = vor.points[p2] - vor.points[p1]\n",
        "                t = t / np.linalg.norm(t)\n",
        "                n = np.array([-t[1], t[0]])\n",
        "                midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
        "                far_point = midpoint + n * radius\n",
        "                new_vertices.append(far_point.tolist())\n",
        "                new_region.append(len(new_vertices) - 1)\n",
        "\n",
        "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
        "        c = vs.mean(axis=0)\n",
        "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
        "        new_region = np.array(new_region)[np.argsort(angles)]\n",
        "        new_regions.append(new_region.tolist())\n",
        "\n",
        "    return new_regions, np.asarray(new_vertices)\n"
      ],
      "metadata": {
        "id": "-T3RKj6z8Z6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSK-GI0Knurk"
      },
      "source": [
        "## Section 2.2: Understanding Decision Boundaries  \n",
        "So you have seen the Voronoi diagram of the dataset, implemented KNN, and also seen your algorithm's performance in terms of accuracy? Impressive!  \n",
        "Wouldn't it also be great to know how exactly these 'votes' or neighbours are decided through some kind of visualization?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytP_AImALiZX"
      },
      "source": [
        "### Decision Boundary\n",
        "\n",
        "While the Voronoi diagram gave us a good idea of the points present in our dataset, to understand how KNN performed on our dataset we can plot decision boundaries. Decision boundaries, as the name suggests, divide the plane into different regions of classification.  \n",
        "\n",
        "Note that here again, for simplicity, we have only considered first two attributes of the DataFrame (ie, Sepal Length and Sepal Width).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P8Pqav4DI4N"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def decision_boundary_plot(x_dec,y_dec,k):\n",
        "  h = .02  # step size in the mesh\n",
        "\n",
        "  # Create color maps\n",
        "  n = len(set(y_dec))\n",
        "  cmap_light = ListedColormap(['pink', 'green', 'cyan','yellow'][:n])\n",
        "  cmap_bold = ['pink', 'darkgreen', 'blue','yellow'][:n]\n",
        "\n",
        "  for weights in ['uniform', 'distance']:\n",
        "      # we create an instance of Neighbours Classifier and fit the data.\n",
        "      clf = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
        "      clf.fit(x_dec, y_dec)\n",
        "\n",
        "      # Plot the decision boundary. For that, we will assign a color to each\n",
        "      # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "      x_min, x_max = x_dec[:, 0].min() - 1, x_dec[:, 0].max() + 1\n",
        "      y_min, y_max = x_dec[:, 1].min() - 1, x_dec[:, 1].max() + 1\n",
        "      xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                          np.arange(y_min, y_max, h))\n",
        "      Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "      # Put the result into a color plot\n",
        "      Z = Z.reshape(xx.shape)\n",
        "      plt.figure(figsize=(8, 6))\n",
        "      plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "      # Plot also the training points\n",
        "      sns.scatterplot(x=x_dec[:, 0], y=x_dec[:, 1], hue=y_dec,\n",
        "                      palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
        "      plt.xlim(xx.min(), xx.max())\n",
        "      plt.ylim(yy.min(), yy.max())\n",
        "      plt.title(\"Multi-Classification (k = %i, weights = '%s')\"% (k, weights))\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgAO62vWKPHt"
      },
      "outputs": [],
      "source": [
        "x_pts = X[:,:2]\n",
        "y_pts = y\n",
        "decision_boundary_plot(x_pts,y_pts,7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFRJidIgr6gt"
      },
      "outputs": [],
      "source": [
        "## TASK-2\n",
        "## In the above cells, we saw the Voronoi diagram of the data and plotted the KNN decision boundaries\n",
        "## by only considering two attributes of the dataset. You must be already familiar with PCA.\n",
        "## Apply PCA on the dataset above to reduce it to two dimensions.\n",
        "## Plot the Voronoi diagram and Decision boundaries after that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "\n",
        "# Add labels\n",
        "df[\"target\"] = iris.target\n",
        "\n",
        "# Create X (features) and y (labels)\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "print(\"df shape:\", df.shape)\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Original shape:\", X.shape)\n",
        "print(\"After PCA:\", X_pca.shape)\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vor = Voronoi(X_pca)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "voronoi_plot_2d(vor, show_vertices=False, show_points=True)\n",
        "plt.title(\"Voronoi Diagram After PCA\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train KNN on PCA-reduced data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_pca, y)\n",
        "\n",
        "# Mesh grid for decision regions\n",
        "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 300),\n",
        "    np.linspace(y_min, y_max, 300)\n",
        ")\n",
        "\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap='Pastel1')\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', edgecolor='k')\n",
        "\n",
        "plt.title(\"KNN Decision Boundaries After PCA\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6EsiyGIx5Mfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA for Dimensionality Reduction\n",
        "Principal Component Analysis (PCA) is a technique that reduces a dataset with many dimensions into fewer dimensions while preserving most of the variance. It works by:\n",
        "1. Finding the directions where the data varies the most.\n",
        "2. Projecting the data onto those directions (principal components).\n",
        "For this task, we reduce the dataset to **two dimensions** so we can plot Voronoi diagrams and KNN decision boundaries.\n"
      ],
      "metadata": {
        "id": "Ru_RibH54RLb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gti-Jgu_VBDl"
      },
      "source": [
        "## Section 2.3: Confusion Matrix  \n",
        "In classification problems, a confusion matrix, also known as an error matrix, is a table that allows visualization of the performance of an algorithm, typically a supervised learning one. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPfc8YFBA8Oh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMXIM9apA99s"
      },
      "outputs": [],
      "source": [
        "# print(confusion_matrix(y_test,predictions))\n",
        "pd.crosstab(y_test, predictions, rownames=['True'], colnames=['Predicted'], margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cgYG0E5UHdy"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"viridis\" ,fmt='g')\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTPQOPStVtmI"
      },
      "source": [
        "## Section 2.4: Classification Report\n",
        "\n",
        "Precision, Recall, and F1-Score are other metrics besides accuracy that one might look for in an algorithm.  Depending on the use-case, one might consider one metric more important than the other.  \n",
        "\n",
        "Note: *T-> True, F->False, P->Positive, N->Negative*\n",
        "    \n",
        "Mathematically, Accuracy is :  \n",
        "\n",
        "$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$  \n",
        "\n",
        "**Precision**: The accuracy of positive predictions\n",
        "\n",
        "\n",
        "$Precision = \\frac{TP}{TP+FP}$\n",
        "\n",
        "**Recall**:Fraction of positives that were correctly identified\n",
        "\n",
        "\n",
        "$Recall = \\frac{TP}{TP+FN}$\n",
        "\n",
        "\n",
        "**F1-score**: Harmonic mean of precision and recall  \n",
        "\n",
        "\n",
        "$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH3KEfEYW190"
      },
      "outputs": [],
      "source": [
        "#import classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtYNFoJh0LU-"
      },
      "source": [
        "### **Car Evaluation Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsBukCMi4UjJ"
      },
      "outputs": [],
      "source": [
        "# Upload the Car evaluation data CSV file that has been shared with you.\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T5yvzVH4jrQ"
      },
      "outputs": [],
      "source": [
        "car_df = pd.read_csv('car_evaluation.csv')\n",
        "car_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwjBankjB9Os"
      },
      "outputs": [],
      "source": [
        "for x in car_df.columns:\n",
        "  # print(x)\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(car_df[x])\n",
        "  car_df[x]=le.transform(car_df[x])\n",
        "\n",
        "car_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Klx4yMCgKa"
      },
      "outputs": [],
      "source": [
        "dataset = car_df.values\n",
        "X = dataset[:,0:6]\n",
        "y = np.array(dataset[:,6])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HL5ufCHDANh"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "predictions = knn.predict(X_test)\n",
        "score = accuracy_score(y_test, predictions)\n",
        "print(\"The accuracy of the classifier on Car evaluation dataset is {:.2f} %\".format(100*score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYCimUC_A56C"
      },
      "outputs": [],
      "source": [
        "## TASK-3\n",
        "## Plot a Confusion Matrix for the results of the Car evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
        "columns = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
        "\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "df.head()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df_encoded = df.copy()\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in df_encoded.columns:\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "\n",
        "df_encoded.head()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_encoded.drop(\"class\", axis=1)\n",
        "y = df_encoded[\"class\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=le.fit(df[\"class\"]).classes_)\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Car Evaluation Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xRPIjnQu5c-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8TRknp0XTqJ"
      },
      "outputs": [],
      "source": [
        "## TASK-4\n",
        "## Print a Classification Report for the results of the Car evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ETM2Oi5G5puS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJFiD9dzNt-H"
      },
      "outputs": [],
      "source": [
        "## TASK-5\n",
        "## Plot the Decision boundary diagram for the classifier of the Car evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Fit KNN again on PCA data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create meshgrid\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 300),\n",
        "    np.linspace(y_min, y_max, 300)\n",
        ")\n",
        "\n",
        "# Predict on grid\n",
        "Z = knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=\"Pastel1\")\n",
        "\n",
        "# Plot actual PCA-transformed points\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1],\n",
        "            c=y_train, cmap=\"Set1\", edgecolor=\"k\")\n",
        "\n",
        "plt.title(\"Decision Boundary after PCA (Car Evaluation Dataset)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A70BDj6r5xz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUVQ-oD6OA8j"
      },
      "outputs": [],
      "source": [
        "## TASK-6\n",
        "## Plot the Voronoi diagram for the classifier of the Car evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce full dataset to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)  # Use full X (not train only)\n",
        "\n",
        "# Train classifier on PCA-reduced data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_pca, y)\n",
        "\n",
        "print(\"PCA output shape:\", X_pca.shape)\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create Voronoi structure\n",
        "vor = Voronoi(X_pca)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "voronoi_plot_2d(vor, show_vertices=False, line_colors=\"black\", line_width=1)\n",
        "\n",
        "# Scatter actual data points with class colors\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"Set1\", edgecolor=\"k\")\n",
        "\n",
        "plt.title(\"Voronoi Diagram After PCA (Car Evaluation Dataset)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8NhvFuid588R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Homework Assignment: Exploring Distance Metrics and KNN**\n",
        "\n",
        "---\n",
        "\n",
        "### **Problem 1: Implementing Custom Distance Metrics**\n",
        "\n",
        "In this exercise, you'll implement distance metrics from scratch to deepen your understanding.\n",
        "\n",
        "**Task 1.1:** Implement the Minkowski distance formula without using any library functions (except basic numpy operations).\n",
        "\n",
        "Recall that Minkowski distance is defined as:\n",
        "\n",
        "$$d(x, x') = \\left(\\sum_{j=1}^{D} |x_j - x'_j|^p\\right)^{1/p}$$\n",
        "\n",
        "where $p$ is a parameter. When $p=1$, it's Manhattan distance, and when $p=2$, it's Euclidean distance."
      ],
      "metadata": {
        "id": "pACMj6YcKdaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## HOMEWORK TASK 1.1\n",
        "## Implement the Minkowski distance function below\n",
        "## Test it with p=1 (should match Manhattan) and p=2 (should match Euclidean)\n",
        "## Expected number of lines: 3-5\n",
        "\n",
        "def minkowski_distance(x1, x2, p):\n",
        "    \"\"\"\n",
        "    Calculate Minkowski distance between two points.\n",
        "\n",
        "    Parameters:\n",
        "    x1, x2: numpy arrays representing points\n",
        "    p: the order of the Minkowski distance\n",
        "\n",
        "    Returns:\n",
        "    float: the Minkowski distance\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "x_1 = np.array([1, 2, 3])\n",
        "x_2 = np.array([4, 6, 8])\n",
        "\n",
        "# Test with p=1 (Manhattan)\n",
        "print(\"Minkowski distance (p=1):\", minkowski_distance(x_1, x_2, 1))\n",
        "print(\"Manhattan distance:\", distance.cityblock(x_1, x_2))\n",
        "print()\n",
        "\n",
        "# Test with p=2 (Euclidean)\n",
        "print(\"Minkowski distance (p=2):\", minkowski_distance(x_1, x_2, 2))\n",
        "print(\"Euclidean distance:\", distance.euclidean(x_1, x_2))"
      ],
      "metadata": {
        "id": "rC3TRvUVKe6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minkowski_distance(x1, x2, p):\n",
        "    return np.sum(np.abs(x1 - x2) ** p) ** (1 / p)\n",
        "# Test your implementation\n",
        "x_1 = np.array([1, 2, 3])\n",
        "x_2 = np.array([4, 6, 8])\n",
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Test with p=1 (Manhattan)\n",
        "print(\"Minkowski distance (p=1):\", minkowski_distance(x_1, x_2, 1))\n",
        "print(\"Manhattan distance:\", distance.cityblock(x_1, x_2))\n",
        "print()\n",
        "\n",
        "# Test with p=2 (Euclidean)\n",
        "print(\"Minkowski distance (p=2):\", minkowski_distance(x_1, x_2, 2))\n",
        "print(\"Euclidean distance:\", distance.euclidean(x_1, x_2))\n"
      ],
      "metadata": {
        "id": "rYue2fGl6YAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Problem 2: Visualizing Decision Boundaries**\n",
        "\n",
        "Understanding how KNN makes predictions is crucial. In this exercise, you'll visualize decision boundaries for different values of K.\n",
        "\n",
        "**Task 2.1:** Create a function that visualizes the decision boundary of a KNN classifier on 2D data.\n",
        "\n",
        "**Hint:** You'll need to:\n",
        "1. Create a mesh grid covering the feature space\n",
        "2. Train a KNN classifier\n",
        "3. Predict the class for each point in the mesh\n",
        "4. Plot the results using `plt.contourf()` for the decision boundary\n",
        "5. Overlay the training data points"
      ],
      "metadata": {
        "id": "8JH8NUriKnos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## HOMEWORK TASK 2.1\n",
        "## Complete the function below to visualize KNN decision boundaries\n",
        "## The function should create a mesh grid and predict class for each point\n",
        "\n",
        "def plot_decision_boundary(X, y, k_value, metric='euclidean'):\n",
        "    \"\"\"\n",
        "    Plot the decision boundary for KNN classifier.\n",
        "\n",
        "    Parameters:\n",
        "    X: Training data (n_samples, 2)\n",
        "    y: Training labels\n",
        "    k_value: Number of neighbors for KNN\n",
        "    metric: Distance metric to use\n",
        "    \"\"\"\n",
        "    # Create a mesh grid\n",
        "    h = 0.02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Train KNN classifier\n",
        "    # Write your code here to create and train KNN model\n",
        "\n",
        "    # Predict for each point in mesh\n",
        "    # Write your code here to make predictions\n",
        "\n",
        "    # Plot decision boundary and training points\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Write your code here to plot contours and scatter points\n",
        "\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(f'KNN Decision Boundary (k={k_value}, metric={metric})')\n",
        "    plt.show()\n",
        "\n",
        "# Generate synthetic data\n",
        "n = 50\n",
        "pts, tgts = generate_synth_data(n)\n",
        "\n",
        "# Test with different k values\n",
        "for k in [1, 3, 5, 10]:\n",
        "    plot_decision_boundary(pts, tgts, k)"
      ],
      "metadata": {
        "id": "YZvD6NJaK0hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synth_data(n_samples=50):\n",
        "    \"\"\"\n",
        "    Generate simple 2D synthetic classification data.\n",
        "    Returns:\n",
        "        X: (n_samples, 2) feature matrix\n",
        "        y: labels (0 or 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Class 0\n",
        "    x1 = np.random.normal(loc=[2, 2], scale=0.7, size=(n_samples, 2))\n",
        "    y1 = np.zeros(n_samples)\n",
        "\n",
        "    # Class 1\n",
        "    x2 = np.random.normal(loc=[5, 5], scale=0.7, size=(n_samples, 2))\n",
        "    y2 = np.ones(n_samples)\n",
        "\n",
        "    # Combine\n",
        "    X = np.vstack((x1, x2))\n",
        "    y = np.hstack((y1, y2))\n",
        "\n",
        "    return X, y\n",
        "n = 50\n",
        "pts, tgts = generate_synth_data(n)\n",
        "\n",
        "for k in [1, 3, 5, 10]:\n",
        "    plot_decision_boundary(pts, tgts, k)\n"
      ],
      "metadata": {
        "id": "-EQU9mzr7GQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion Questions:**\n",
        "1. What happens to the decision boundary as K increases? Is it smoother or more irregular?\n",
        "2. Which K value seems to generalize better without overfitting?\n",
        "3. Try changing the metric to 'manhattan' - how does the boundary change?\n",
        "\n",
        "---\n",
        "\n",
        "### **Problem 3: Impact of Distance Metrics on High-Dimensional Data**\n",
        "\n",
        "The \"curse of dimensionality\" affects distance metrics differently. Let's explore this phenomenon.\n",
        "\n",
        "**Task 3.1:** Create a function that generates random high-dimensional data and compares how different distance metrics behave as dimensionality increases.\n",
        "\n",
        "**Background:** In high dimensions, the ratio between the maximum and minimum distances between points tends to approach 1, making it harder to distinguish between near and far neighbors. This affects different metrics differently."
      ],
      "metadata": {
        "id": "nI6WEtUQK40c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## HOMEWORK TASK 3.1\n",
        "## Complete the function to analyze distance metrics in high dimensions\n",
        "## Calculate the ratio of max distance to min distance for each metric\n",
        "def analyze_distance_in_high_dims(max_dim=50, n_points=100):\n",
        "    \"\"\"\n",
        "    Analyze how distance metrics behave in high dimensions.\n",
        "\n",
        "    Parameters:\n",
        "    max_dim: Maximum dimensionality to test\n",
        "    n_points: Number of random points to generate\n",
        "\n",
        "    Returns:\n",
        "    Dictionary with results for each metric\n",
        "    \"\"\"\n",
        "    dimensions = range(2, max_dim+1, 5)\n",
        "    results = {\n",
        "        'euclidean': [],\n",
        "        'manhattan': [],\n",
        "        'chebyshev': []\n",
        "    }\n",
        "\n",
        "    for dim in dimensions:\n",
        "        # Generate random points\n",
        "        points = np.random.randn(n_points, dim)\n",
        "\n",
        "        # Calculate all pairwise distances for each metric\n",
        "        # Write your code here to compute distances and calculate ratio\n",
        "\n",
        "        # Store the ratio of max/min distance\n",
        "        pass\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6)) # Write your code here to plot the results\n",
        "# Run the analysis\n",
        "results = analyze_distance_in_high_dims()\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ze7BmrQfK63r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_distance_in_high_dims(max_dim=50, n_points=100):\n",
        "    dimensions = range(2, max_dim+1, 5)\n",
        "    results = {\n",
        "        'euclidean': [],\n",
        "        'manhattan': [],\n",
        "        'chebyshev': []\n",
        "    }\n",
        "\n",
        "    for dim in dimensions:\n",
        "        # Generate random points\n",
        "        points = np.random.randn(n_points, dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        dist_euclid = pdist(points, metric='euclidean')\n",
        "        dist_manh   = pdist(points, metric='cityblock')\n",
        "        dist_cheby  = pdist(points, metric='chebyshev')\n",
        "\n",
        "        # Ratios\n",
        "        ratio_e = dist_euclid.max() / dist_euclid.min()\n",
        "        ratio_m = dist_manh.max()   / dist_manh.min()\n",
        "        ratio_c = dist_cheby.max()  / dist_cheby.min()\n",
        "\n",
        "        # Store results\n",
        "        results['euclidean'].append(ratio_e)\n",
        "        results['manhattan'].append(ratio_m)\n",
        "        results['chebyshev'].append(ratio_c)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(dimensions, results['euclidean'], label='Euclidean', marker='o')\n",
        "    plt.plot(dimensions, results['manhattan'], label='Manhattan', marker='s')\n",
        "    plt.plot(dimensions, results['chebyshev'], label='Chebyshev', marker='^')\n",
        "    plt.xlabel(\"Dimensionality\")\n",
        "    plt.ylabel(\"Max Distance / Min Distance\")\n",
        "    plt.title(\"Effect of Dimensionality on Different Distance Metrics\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "TPKMOktm9WOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr9dI1Kqsprz"
      },
      "source": [
        "### Questions to Think About and Answer\n",
        "1. In the section on Decision boundaries, you must have seen that we ran the KNN algorithm twice: first with the _weights_ set to 'uniform' and then set to 'distance'. Find out the difference between these two.  \n",
        "2. What do you think could be the drawbacks of using KNN ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference between uniform and distance weights in KNN\n",
        "Uniform weights (weights='uniform'):\n",
        "All k neighbours are treated equally. Each neighbour gets one equal vote, no matter how close or far it is from the test point. The prediction is based on simple majority voting.\n",
        "Distance weights (weights='distance'):\n",
        "Closer neighbours have more influence than farther ones. Votes are weighted by how close a point is, so points very near the test sample dominate the prediction.\n",
        "Simple difference:\n",
        "Uniform = every neighbour counts the same.\n",
        "Distance = closer neighbours matter more.\n",
        "Drawbacks of using KNN\n",
        "Computationally expensive:\n",
        "KNN has no training step, so during prediction it must compare the test point with every training point. This makes it slow for large datasets.\n",
        "High memory usage:\n",
        "KNN must store the entire training dataset, because it uses the raw data for predictions.\n",
        "Sensitive to irrelevant or unscaled features:\n",
        "Distance calculations can become misleading if features are not normalized or if many features are unnecessary.\n",
        "Curse of dimensionality:\n",
        "As the number of features increases, distances between points become very similar, making it hard for KNN to correctly identify nearest neighbours. This reduces accuracy."
      ],
      "metadata": {
        "id": "s5b72YAU9vvd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oybMw8WM9oFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRlrn4EctZrC"
      },
      "source": [
        "### Useful Resources for further reading\n",
        "1. Interactive KNN visualization, with class boundaries: http://vision.stanford.edu/teaching/cs231n-demos/knn/  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}